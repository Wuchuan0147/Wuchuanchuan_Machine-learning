import streamlit as st
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.font_manager as fm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, learning_curve, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (confusion_matrix, roc_curve, auc, precision_recall_curve, 
                             classification_report, roc_auc_score, average_precision_score,
                             f1_score, accuracy_score, precision_score, recall_score)
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from scipy.stats import randint, uniform
import time
import warnings
import joblib
import os
import io
import base64
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“
try:
    font_list = [f.name for f in fm.fontManager.ttflist]
    chinese_fonts = ['SimHei', 'Microsoft YaHei', 'STSong', 'SimSun', 'Arial Unicode MS', 'Heiti SC']
    
    available_font = None
    for font in chinese_fonts:
        if font in font_list:
            available_font = font
            break
    
    if available_font:
        plt.rcParams['font.sans-serif'] = [available_font]
        mpl.rcParams['font.sans-serif'] = [available_font]
    else:
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        mpl.rcParams['font.sans-serif'] = ['DejaVu Sans']
    
    plt.rcParams['axes.unicode_minus'] = False
    mpl.rcParams['axes.unicode_minus'] = False
except Exception as e:
    st.warning(f"å­—ä½“è®¾ç½®é”™è¯¯: {e}")

# å…¨å±€ç»˜å›¾å‚æ•°è®¾ç½®
plt.rcParams.update({
    'font.size': 14, 'axes.labelsize': 14, 'xtick.labelsize': 13, 
    'ytick.labelsize': 13, 'legend.fontsize': 12, 'figure.figsize': (10, 8),
    'figure.titlesize': 16, 'axes.titlesize': 16, 'savefig.dpi': 300,
    'savefig.bbox': 'tight'
})

warnings.filterwarnings('ignore')

# è®¾ç½®é¡µé¢
st.set_page_config(
    page_title="æœºå™¨å­¦ä¹ åˆ¤åˆ«åˆ†ç±»æ¨¡å‹ç³»ç»Ÿ",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è·å–å½“å‰ç›®å½•
script_dir = os.path.dirname(os.path.abspath(__file__))
os.chdir(script_dir)

# åˆå§‹åŒ–session state
def initialize_session_state():
    if 'trained_models' not in st.session_state:
        st.session_state.trained_models = {}
    if 'data_loaded' not in st.session_state:
        st.session_state.data_loaded = False
    if 'X_train' not in st.session_state:
        st.session_state.X_train = None
    if 'X_test' not in st.session_state:
        st.session_state.X_test = None
    if 'y_train' not in st.session_state:
        st.session_state.y_train = None
    if 'y_test' not in st.session_state:
        st.session_state.y_test = None
    if 'feature_names' not in st.session_state:
        st.session_state.feature_names = None
    if 'current_page' not in st.session_state:
        st.session_state.current_page = "é¦–é¡µ"
    if 'uploaded_file' not in st.session_state:
        st.session_state.uploaded_file = None
    if 'custom_params' not in st.session_state:
        st.session_state.custom_params = {}
    if 'training_complete' not in st.session_state:
        st.session_state.training_complete = False
    if 'evaluation_results' not in st.session_state:
        st.session_state.evaluation_results = None
    if 'cv_folds' not in st.session_state:
        st.session_state.cv_folds = 5  # é»˜è®¤5æŠ˜äº¤å‰éªŒè¯
    if 'data_preprocessed' not in st.session_state:
        st.session_state.data_preprocessed = False
    if 'models_trained' not in st.session_state:
        st.session_state.models_trained = False
    if 'evaluation_done' not in st.session_state:
        st.session_state.evaluation_done = False

# è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºä¸‹è½½é“¾æ¥
def get_table_download_link(df, filename, link_text):
    """ç”Ÿæˆè¡¨æ ¼ä¸‹è½½é“¾æ¥"""
    csv = df.to_csv(index=True)
    b64 = base64.b64encode(csv.encode()).decode()
    href = f'<a href="data:file/csv;base64,{b64}" download="{filename}">{link_text}</a>'
    return href

def get_image_download_link(fig, filename, link_text):
    """ç”Ÿæˆå›¾ç‰‡ä¸‹è½½é“¾æ¥"""
    buf = io.BytesIO()
    fig.savefig(buf, format='png', dpi=300, bbox_inches='tight')
    buf.seek(0)
    img_str = base64.b64encode(buf.read()).decode()
    href = f'<a href="data:image/png;base64,{img_str}" download="{filename}">{link_text}</a>'
    return href

def create_evaluation_report(metrics_df, rank_df, models, feature_names, cv_folds):
    """åˆ›å»ºè¯„ä¼°æŠ¥å‘Š"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_content = []
    
    # æŠ¥å‘Šæ ‡é¢˜
    report_content.append(f"æœºå™¨å­¦ä¹ åˆ¤åˆ«åˆ†ç±»æ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
    report_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_content.append(f"äº¤å‰éªŒè¯æŠ˜æ•°: {cv_folds}")
    report_content.append("="*50)
    report_content.append("")
    
    # æ€§èƒ½æŒ‡æ ‡
    report_content.append("æ¨¡å‹æ€§èƒ½æŒ‡æ ‡æ±‡æ€»:")
    report_content.append("")
    for model_name in metrics_df.index:
        report_content.append(f"{model_name}:")
        report_content.append(f"  å‡†ç¡®ç‡: {metrics_df.loc[model_name, 'Accuracy']:.4f}")
        report_content.append(f"  ç²¾ç¡®ç‡: {metrics_df.loc[model_name, 'Precision']:.4f}")
        report_content.append(f"  å¬å›ç‡: {metrics_df.loc[model_name, 'Recall']:.4f}")
        report_content.append(f"  F1åˆ†æ•°: {metrics_df.loc[model_name, 'F1']:.4f}")
        report_content.append(f"  ROC AUC: {metrics_df.loc[model_name, 'ROC_AUC']:.4f}")
        report_content.append(f"  PR AUC: {metrics_df.loc[model_name, 'PR_AUC']:.4f}")
        report_content.append("")
    
    # æ’åç»“æœ
    report_content.append("æ¨¡å‹æ€§èƒ½æ’å (1=æœ€ä½³):")
    for model_name in rank_df.index:
        report_content.append(f"{model_name}: å¹³å‡æ’å {rank_df.loc[model_name, 'Average_Rank']:.2f}")
    
    # æœ€ä½³æ¨¡å‹
    best_model = rank_df['Average_Rank'].idxmin()
    report_content.append("")
    report_content.append(f"æœ€ä½³æ¨¡å‹: {best_model}")
    report_content.append(f"æœ€ä½³å‚æ•°: {models[best_model]['best_params']}")
    
    return "\n".join(report_content), timestamp

# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†å‡½æ•°
def load_and_preprocess_data(uploaded_file):
    try:
        # è¯»å–æ•°æ®
        if uploaded_file.name.endswith('.xlsx'):
            data = pd.read_excel(uploaded_file)
        else:
            data = pd.read_csv(uploaded_file)
        
        # å¤„ç†ç¼ºå¤±å€¼
        if data.isnull().sum().any():
            data = data.dropna()
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
        X = data.iloc[:, :-1]
        feature_names = X.columns.tolist()
        y = data['Label']
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        joblib.dump(scaler, 'scaler.pkl')
        
        return X_train, X_test, y_train, y_test, feature_names, data.shape
    except Exception as e:
        st.error(f"æ•°æ®åŠ è½½å’Œé¢„å¤„ç†é”™è¯¯: {e}")
        return None, None, None, None, None, None

# æ¨¡å‹è®­ç»ƒå‡½æ•°
def train_models(X_train, y_train, selected_models, search_method, cv_folds, custom_params=None):
    # å®šä¹‰æ¨¡å‹é…ç½®
    base_models = {
        'XGBoost': {
            'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
            'random_params': {
                'n_estimators': randint(50, 300),
                'max_depth': randint(3, 10),
                'learning_rate': uniform(0.01, 0.3),
                'subsample': uniform(0.6, 0.4),
                'colsample_bytree': uniform(0.6, 0.4)
            },
            'grid_params': {
                'n_estimators': [50, 100, 200, 300],
                'max_depth': [3, 5, 7, 10],
                'learning_rate': [0.01, 0.1, 0.2, 0.3],
                'subsample': [0.6, 0.8, 1.0],
                'colsample_bytree': [0.6, 0.8, 1.0]
            }
        },
        'Random Forest': {
            'model': RandomForestClassifier(random_state=42),
            'random_params': {
                'n_estimators': randint(50, 300),
                'max_depth': randint(3, 20),
                'min_samples_split': randint(2, 10),
                'min_samples_leaf': randint(1, 5)
            },
            'grid_params': {
                'n_estimators': [50, 100, 200, 300],
                'max_depth': [3, 5, 10, 15, 20],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 5]
            }
        },
        'SVM': {
            'model': SVC(probability=True, random_state=42),
            'random_params': {
                'C': uniform(0.1, 10),
                'gamma': uniform(0.01, 1),
                'kernel': ['linear', 'rbf', 'poly']
            },
            'grid_params': {
                'C': [0.1, 1, 10, 100],
                'gamma': [0.01, 0.1, 1, 10],
                'kernel': ['linear', 'rbf', 'poly']
            }
        },
        'Logistic Regression': {
            'model': LogisticRegression(max_iter=1000, random_state=42),
            'random_params': {
                'C': uniform(0.1, 10),
                'penalty': ['l1', 'l2'],
                'solver': ['liblinear']
            },
            'grid_params': {
                'C': [0.1, 1, 10, 100],
                'penalty': ['l1', 'l2'],
                'solver': ['liblinear']
            }
        },
        'Neural Network': {
            'model': MLPClassifier(max_iter=1000, random_state=42),
            'random_params': {
                'hidden_layer_sizes': [(50,), (100,), (50, 30), (100, 50)],
                'alpha': uniform(0.0001, 0.1),
                'activation': ['relu', 'tanh'],
                'learning_rate_init': uniform(0.001, 0.01)
            },
            'grid_params': {
                'hidden_layer_sizes': [(50,), (100,), (50, 30), (100, 50)],
                'alpha': [0.0001, 0.001, 0.01, 0.1],
                'activation': ['relu', 'tanh'],
                'learning_rate_init': [0.001, 0.01, 0.1]
            }
        }
    }
    
    # å®šä¹‰è¯„ä¼°æŒ‡æ ‡
    scoring_metrics = {
        'accuracy': 'accuracy',
        'precision': 'precision_macro',
        'recall': 'recall_macro',
        'f1': 'f1_macro',
        'roc_auc': 'roc_auc_ovo'
    }
    
    # åªè®­ç»ƒé€‰ä¸­çš„æ¨¡å‹
    models = {name: config for name, config in base_models.items() if name in selected_models}
    
    # è®­ç»ƒæ¨¡å‹
    trained_models = {}
    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, (name, config) in enumerate(models.items()):
        status_text.text(f"è®­ç»ƒ {name}... (ä½¿ç”¨{cv_folds}æŠ˜äº¤å‰éªŒè¯)")
        
        # ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°æˆ–é»˜è®¤å‚æ•°
        if custom_params and name in custom_params:
            params = custom_params[name]
            model = config['model']
            model.set_params(**params)
            model.fit(X_train, y_train)
            
            model_dict = {
                'model': model,
                'best_params': params,
                'cv_metrics': {},
                'train_metrics': {},
                'cv_folds': cv_folds
            }
        else:
            # ä½¿ç”¨æœç´¢æ–¹æ³•
            if search_method == "éšæœºæœç´¢":
                search = RandomizedSearchCV(
                    config['model'], 
                    config['random_params'], 
                    n_iter=20,
                    cv=cv,
                    scoring=scoring_metrics,
                    refit='f1',
                    n_jobs=-1,
                    random_state=42,
                    return_train_score=True
                )
            else:  # ç½‘æ ¼æœç´¢
                search = GridSearchCV(
                    config['model'], 
                    config['grid_params'], 
                    cv=cv,
                    scoring=scoring_metrics,
                    refit='f1',
                    n_jobs=-1,
                    return_train_score=True
                )
            
            search.fit(X_train, y_train)
            
            # æå–äº¤å‰éªŒè¯æ€§èƒ½æŒ‡æ ‡
            best_index = search.best_index_
            cv_metrics = {}
            for metric in scoring_metrics.keys():
                mean_key = f'mean_test_{metric}'
                std_key = f'std_test_{metric}'
                if mean_key in search.cv_results_ and std_key in search.cv_results_:
                    cv_metrics[metric] = {
                        'mean': search.cv_results_[mean_key][best_index],
                        'std': search.cv_results_[std_key][best_index]
                    }
            
            # è®¡ç®—è®­ç»ƒé›†æ€§èƒ½æŒ‡æ ‡
            model = search.best_estimator_
            y_train_pred = model.predict(X_train)
            train_metrics = {
                'accuracy': accuracy_score(y_train, y_train_pred),
                'precision': precision_score(y_train, y_train_pred, average='weighted'),
                'recall': recall_score(y_train, y_train_pred, average='weighted'),
                'f1': f1_score(y_train, y_train_pred, average='weighted')
            }
            
            model_dict = {
                'model': model,
                'best_params': search.best_params_,
                'cv_metrics': cv_metrics,
                'train_metrics': train_metrics,
                'cv_folds': cv_folds
            }
        
        # ä¿å­˜æ¨¡å‹
        model_filename = f"{name.replace(' ', '_')}_model.pkl"
        joblib.dump(model_dict, model_filename)
        
        trained_models[name] = model_dict
        
        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.progress((i + 1) / len(models))
    
    status_text.text("è®­ç»ƒå®Œæˆ!")
    st.session_state.training_complete = True
    st.session_state.models_trained = True
    return trained_models

# ç»˜åˆ¶æ··æ·†çŸ©é˜µ
def plot_confusion_matrices(models, X_train, X_test, y_train, y_test):
    figures = {}
    for model_name, model_dict in models.items():
        model = model_dict['model']
        
        # è®­ç»ƒé›†æ··æ·†çŸ©é˜µ
        y_train_pred = model.predict(X_train)
        cm_train = confusion_matrix(y_train, y_train_pred)
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=['éæˆçŸ¿', 'æˆçŸ¿'],
                    yticklabels=['éæˆçŸ¿', 'æˆçŸ¿'],
                    annot_kws={"size": 14},
                    cbar=False, ax=ax1)
        ax1.set_title(f'{model_name} - è®­ç»ƒé›†æ··æ·†çŸ©é˜µ', fontsize=14)
        ax1.set_ylabel('å®é™…ç±»åˆ«', fontsize=12)
        ax1.set_xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
        
        # æµ‹è¯•é›†æ··æ·†çŸ©é˜µ
        y_test_pred = model.predict(X_test)
        cm_test = confusion_matrix(y_test, y_test_pred)
        
        sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['éæˆçŸ¿', 'æˆçŸ¿'],
                    yticklabels=['éæˆçŸ¿', 'æˆçŸ¿'],
                    annot_kws={"size": 14},
                    cbar=False, ax=ax2)
        ax2.set_title(f'{model_name} - æµ‹è¯•é›†æ··æ·†çŸ©é˜µ', fontsize=14)
        ax2.set_ylabel('å®é™…ç±»åˆ«', fontsize=12)
        ax2.set_xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
        
        plt.tight_layout()
        figures[model_name] = fig
        st.pyplot(fig)
        plt.close()
    
    return figures

# ç»˜åˆ¶ROCæ›²çº¿
def plot_roc_curves(models, X_test, y_test):
    plt.figure(figsize=(10, 8))
    plt.plot([0, 1], [0, 1], 'k--', alpha=0.7, label='éšæœºçŒœæµ‹')
    
    for model_name, model_dict in models.items():
        model = model_dict['model']
        if hasattr(model, "predict_proba"):
            y_prob = model.predict_proba(X_test)[:, 1]
            fpr, tpr, _ = roc_curve(y_test, y_prob)
            roc_auc = auc(fpr, tpr)
            plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')
    
    plt.xlim([-0.02, 1.02])
    plt.ylim([-0.02, 1.02])
    plt.xlabel('å‡æ­£ä¾‹ç‡ (FPR)', fontsize=12)
    plt.ylabel('çœŸæ­£ä¾‹ç‡ (TPR)', fontsize=12)
    plt.title('ROCæ›²çº¿', fontsize=16)
    plt.legend(loc='lower right', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    fig = plt.gcf()
    st.pyplot(fig)
    plt.close()
    return fig

# ç»˜åˆ¶PRæ›²çº¿
def plot_pr_curves(models, X_test, y_test):
    plt.figure(figsize=(10, 8))
    
    # æ·»åŠ éšæœºåŸºå‡†çº¿
    random_precision = sum(y_test) / len(y_test)
    plt.plot([0, 1], [random_precision, random_precision], 'k--', alpha=0.7, label='éšæœºçŒœæµ‹')
    
    for model_name, model_dict in models.items():
        model = model_dict['model']
        if hasattr(model, "predict_proba"):
            y_prob = model.predict_proba(X_test)[:, 1]
            precision, recall, _ = precision_recall_curve(y_test, y_prob)
            avg_precision = average_precision_score(y_test, y_prob)
            plt.plot(recall, precision, lw=2, label=f'{model_name} (AP = {avg_precision:.3f})')
    
    plt.xlim([-0.02, 1.02])
    plt.ylim([-0.02, 1.02])
    plt.xlabel('å¬å›ç‡ (Recall)', fontsize=12)
    plt.ylabel('ç²¾ç¡®ç‡ (Precision)', fontsize=12)
    plt.title('PRæ›²çº¿', fontsize=16)
    plt.legend(loc='lower right', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    fig = plt.gcf()
    st.pyplot(fig)
    plt.close()
    return fig

# ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§
def plot_feature_importance(models, feature_names):
    figures = {}
    for model_name, model_dict in models.items():
        model = model_dict['model']
        
        # è·³è¿‡é€»è¾‘å›å½’æ¨¡å‹
        if model_name == 'Logistic Regression':
            continue
            
        # å¤„ç†ä¸åŒæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
        if hasattr(model, 'feature_importances_'):
            # æ ‘æ¨¡å‹
            importances = model.feature_importances_
            indices = np.argsort(importances)[::-1]
            
            plt.figure(figsize=(12, 8))
            plt.title(f"{model_name} - ç‰¹å¾é‡è¦æ€§", fontsize=16)
            bars = plt.bar(range(len(importances)), importances[indices], align="center")
            plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90, fontsize=12)
            plt.xlim([-1, len(importances)])
            plt.ylabel("é‡è¦æ€§å¾—åˆ†", fontsize=12)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar in bars:
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height,
                         f'{height:.3f}', ha='center', va='bottom', fontsize=10)
            
            plt.tight_layout()
            fig = plt.gcf()
            figures[model_name] = fig
            st.pyplot(fig)
            plt.close()
            
        elif hasattr(model, 'coef_'):
            # çº¿æ€§æ¨¡å‹ (é™¤é€»è¾‘å›å½’å¤–)
            coef = model.coef_[0]
            indices = np.argsort(np.abs(coef))[::-1]
            
            plt.figure(figsize=(12, 8))
            plt.title(f"{model_name} - ç‰¹å¾ç³»æ•°", fontsize=16)
            bars = plt.bar(range(len(coef)), np.abs(coef)[indices], align="center", color='salmon')
            plt.xticks(range(len(coef)), [feature_names[i] for i in indices], rotation=90, fontsize=12)
            plt.xlim([-1, len(coef)])
            plt.ylabel("ç³»æ•°ç»å¯¹å€¼", fontsize=12)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar in bars:
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height,
                         f'{height:.3f}', ha='center', va='bottom', fontsize=10)
            
            plt.tight_layout()
            fig = plt.gcf()
            figures[model_name] = fig
            st.pyplot(fig)
            plt.close()
    
    return figures

# æ€§èƒ½æŒ‡æ ‡æ’åç³»ç»Ÿ
def calculate_model_ranks(metrics_df):
    # åˆ›å»ºæ’åå‰¯æœ¬
    rank_df = metrics_df.copy()
    
    # ä¸ºæ¯ä¸ªæŒ‡æ ‡è®¡ç®—æ’å (1=æœ€å¥½)
    for column in rank_df.columns:
        # æ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯è¶Šå¤§è¶Šå¥½
        rank_df[column] = rank_df[column].rank(ascending=False)
    
    # è®¡ç®—å¹³å‡æ’å
    rank_df['Average_Rank'] = rank_df.mean(axis=1)
    
    # æŒ‰å¹³å‡æ’åæ’åº
    rank_df = rank_df.sort_values(by='Average_Rank')
    
    return rank_df

# è¯„ä¼°å‡½æ•°
def evaluate_and_visualize(models, X_train, X_test, y_train, y_test, feature_names, cv_folds):
    # å­˜å‚¨è¯„ä¼°æŒ‡æ ‡
    metrics = []
    
    st.subheader("æ¨¡å‹æœ€ä½³å‚æ•°å’Œæ€§èƒ½æ±‡æ€»")
    
    for model_name, model_dict in models.items():
        with st.expander(f"{model_name} è¯¦ç»†ä¿¡æ¯"):
            st.write(f"**æœ€ä½³å‚æ•°:** {model_dict['best_params']}")
            st.write(f"**äº¤å‰éªŒè¯æŠ˜æ•°:** {model_dict.get('cv_folds', cv_folds)}")
            
            # æ‰“å°äº¤å‰éªŒè¯æŒ‡æ ‡
            if 'cv_metrics' in model_dict and model_dict['cv_metrics']:
                st.write("**äº¤å‰éªŒè¯æ€§èƒ½æŒ‡æ ‡:**")
                cv_data = []
                for metric, value in model_dict['cv_metrics'].items():
                    cv_data.append({
                        'æŒ‡æ ‡': metric,
                        'å¹³å‡å€¼': f"{value['mean']:.4f}",
                        'æ ‡å‡†å·®': f"Â± {value['std']:.4f}"
                    })
                st.table(pd.DataFrame(cv_data))
            
            # æ‰“å°è®­ç»ƒé›†æ€§èƒ½æŒ‡æ ‡
            if 'train_metrics' in model_dict and model_dict['train_metrics']:
                st.write("**è®­ç»ƒé›†æ€§èƒ½æŒ‡æ ‡:**")
                train_data = []
                for metric, value in model_dict['train_metrics'].items():
                    train_data.append({
                        'æŒ‡æ ‡': metric,
                        'å€¼': f"{value:.4f}"
                    })
                st.table(pd.DataFrame(train_data))
    
    st.subheader("æ··æ·†çŸ©é˜µ")
    confusion_figures = plot_confusion_matrices(models, X_train, X_test, y_train, y_test)
    
    st.subheader("æµ‹è¯•é›†æ€§èƒ½æŒ‡æ ‡")
    for model_name, model_dict in models.items():
        model = model_dict['model']
        
        # æµ‹è¯•é›†é¢„æµ‹
        y_test_pred = model.predict(X_test)
        
        # ç¡®ä¿æ¨¡å‹æ”¯æŒæ¦‚ç‡é¢„æµ‹
        if hasattr(model, "predict_proba"):
            y_prob = model.predict_proba(X_test)[:, 1]
        else:
            # å¯¹äºä¸æ”¯æŒæ¦‚ç‡é¢„æµ‹çš„æ¨¡å‹ï¼Œä½¿ç”¨å†³ç­–å‡½æ•°
            y_prob = model.decision_function(X_test)
        
        # è®¡ç®—æµ‹è¯•é›†æŒ‡æ ‡
        test_accuracy = accuracy_score(y_test, y_test_pred)
        test_precision = precision_score(y_test, y_test_pred, average='weighted')
        test_recall = recall_score(y_test, y_test_pred, average='weighted')
        test_f1 = f1_score(y_test, y_test_pred, average='weighted')
        
        # è®¡ç®—ROC AUC
        if hasattr(model, "predict_proba") or hasattr(model, "decision_function"):
            roc_auc = roc_auc_score(y_test, y_prob)
        else:
            roc_auc = 0.5  # æ— æ³•è®¡ç®—ROC AUC
        
        # è®¡ç®—PR AUC
        try:
            avg_precision = average_precision_score(y_test, y_prob)
        except:
            avg_precision = 0.0
        
        metrics.append({
            'Model': model_name,
            'Accuracy': test_accuracy,
            'Precision': test_precision,
            'Recall': test_recall,
            'F1': test_f1,
            'ROC_AUC': roc_auc,
            'PR_AUC': avg_precision
        })
        
        # æ˜¾ç¤ºæµ‹è¯•é›†åˆ†ç±»æŠ¥å‘Š
        with st.expander(f"{model_name} æµ‹è¯•é›†åˆ†ç±»æŠ¥å‘Š"):
            st.text(classification_report(y_test, y_test_pred, target_names=['éæˆçŸ¿', 'æˆçŸ¿']))
    
    # åˆ›å»ºæŒ‡æ ‡æ•°æ®æ¡†
    metrics_df = pd.DataFrame(metrics).set_index('Model')
    
    # è®¡ç®—æ’å
    rank_df = calculate_model_ranks(metrics_df)
    
    # æ˜¾ç¤ºæŒ‡æ ‡å’Œæ’å
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**æ¨¡å‹æµ‹è¯•é›†æ€§èƒ½æŒ‡æ ‡:**")
        st.dataframe(metrics_df.style.format("{:.4f}"))
    
    with col2:
        st.write("**æ¨¡å‹æ€§èƒ½æ’å (1=æœ€ä½³):**")
        st.dataframe(rank_df.style.format("{:.2f}"))
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹ (å¹³å‡æ’åæœ€é«˜)
    best_model_name = rank_df['Average_Rank'].idxmin()
    best_model = models[best_model_name]['model']
    st.success(f"**æœ€ä½³æ¨¡å‹: {best_model_name}** (å¹³å‡æ’å {rank_df.loc[best_model_name, 'Average_Rank']:.2f})")
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    joblib.dump(best_model, 'best_model.pkl')
    
    # å¯è§†åŒ–
    st.subheader("ROCæ›²çº¿")
    roc_fig = plot_roc_curves(models, X_test, y_test)
    
    st.subheader("PRæ›²çº¿")
    pr_fig = plot_pr_curves(models, X_test, y_test)
    
    st.subheader("ç‰¹å¾é‡è¦æ€§")
    feature_figures = plot_feature_importance(models, feature_names)
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    evaluation_results = {
        'metrics_df': metrics_df,
        'rank_df': rank_df,
        'confusion_figures': confusion_figures,
        'roc_fig': roc_fig,
        'pr_fig': pr_fig,
        'feature_figures': feature_figures,
        'best_model': best_model_name,
        'models': models,
        'cv_folds': cv_folds
    }
    
    st.session_state.evaluation_results = evaluation_results
    st.session_state.evaluation_done = True
    
    return metrics_df, rank_df, evaluation_results

# é¢„æµ‹å‡½æ•°
def predict_new_dataset(models, uploaded_file, selected_models_for_prediction):
    st.subheader("æ–°æ•°æ®é›†é¢„æµ‹ç»“æœ")
    
    # åŠ è½½æ–°æ•°æ®é›†
    try:
        if uploaded_file.name.endswith('.xlsx'):
            new_data = pd.read_excel(uploaded_file)
        else:
            new_data = pd.read_csv(uploaded_file)
    except Exception as e:
        st.error(f"è¯»å–æ–‡ä»¶é”™è¯¯: {e}")
        return
    
    # æ£€æŸ¥å¹¶å¤„ç†ç¼ºå¤±å€¼
    if new_data.isnull().sum().any():
        st.warning("æ–°æ•°æ®é›†ä¸­å­˜åœ¨ç¼ºå¤±å€¼ï¼Œåˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ...")
        new_data = new_data.dropna()
    
    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨æ ‡ç­¾ï¼‰
    if 'Label' in new_data.columns:
        X_new = new_data.drop('Label', axis=1)
        y_new = new_data['Label']
        has_labels = True
        st.info("æ‰¾åˆ°æ ‡ç­¾åˆ—ï¼Œå°†è®¡ç®—æ€§èƒ½æŒ‡æ ‡ã€‚")
    else:
        X_new = new_data
        has_labels = False
        st.info("æœªæ‰¾åˆ°'Label'åˆ—ï¼Œä»…è¿›è¡Œé¢„æµ‹ã€‚")
    
    # åŠ è½½ä¹‹å‰ä¿å­˜çš„æ ‡å‡†åŒ–å™¨
    try:
        scaler = joblib.load('scaler.pkl')
        X_new_scaled = scaler.transform(X_new)
        st.success("ä½¿ç”¨ä¿å­˜çš„æ ‡å‡†åŒ–å™¨å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ã€‚")
    except FileNotFoundError:
        st.error("é”™è¯¯: æœªæ‰¾åˆ°æ ‡å‡†åŒ–å™¨æ–‡ä»¶'scaler.pkl'ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        return
    
    # å¯¹æ¯ä¸ªé€‰ä¸­çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
    for model_name in selected_models_for_prediction:
        if model_name not in models:
            st.warning(f"æ¨¡å‹ {model_name} æœªè®­ç»ƒï¼Œè·³è¿‡é¢„æµ‹ã€‚")
            continue
            
        st.subheader(f"{model_name} é¢„æµ‹ç»“æœ")
        model_dict = models[model_name]
        model = model_dict['model']
        
        try:
            # è¿›è¡Œé¢„æµ‹
            y_pred = model.predict(X_new_scaled)
            
            # å°è¯•è·å–é¢„æµ‹ç½®ä¿¡åº¦
            confidence = np.ones(len(y_pred))  # é»˜è®¤å€¼
            if hasattr(model, "predict_proba"):
                confidence = np.max(model.predict_proba(X_new_scaled), axis=1)
            elif hasattr(model, "decision_function"):
                decision_values = model.decision_function(X_new_scaled)
                confidence = 1 / (1 + np.exp(-decision_values))  # è½¬æ¢ä¸ºæ¦‚ç‡
            
            # åˆ›å»ºé¢„æµ‹ç»“æœDataFrame
            prediction_df = pd.DataFrame({
                'Predicted_Label': y_pred,
                'Prediction_Confidence': confidence
            })
            
            # æ·»åŠ åŸå§‹ç‰¹å¾
            prediction_df = pd.concat([X_new.reset_index(drop=True), prediction_df], axis=1)
            
            # ä¿å­˜é¢„æµ‹ç»“æœ
            prediction_filename = f"{model_name.replace(' ', '_')}_predictions.csv"
            prediction_df.to_csv(prediction_filename, index=False)
            
            # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
            st.write(f"**é¢„æµ‹ç»“æœç¤ºä¾‹:**")
            st.dataframe(prediction_df.head())
            
            # ä¸‹è½½é¢„æµ‹ç»“æœ
            csv = prediction_df.to_csv(index=False)
            st.download_button(
                label=f"ä¸‹è½½ {model_name} é¢„æµ‹ç»“æœ",
                data=csv,
                file_name=prediction_filename,
                mime="text/csv"
            )
            
            # å¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼Œè®¡ç®—æ€§èƒ½æŒ‡æ ‡
            if has_labels:
                # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                accuracy = accuracy_score(y_new, y_pred)
                precision = precision_score(y_new, y_pred, average='weighted')
                recall = recall_score(y_new, y_pred, average='weighted')
                f1 = f1_score(y_new, y_pred, average='weighted')
                
                # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("å‡†ç¡®ç‡", f"{accuracy:.4f}")
                col2.metric("ç²¾ç¡®ç‡", f"{precision:.4f}")
                col3.metric("å¬å›ç‡", f"{recall:.4f}")
                col4.metric("F1åˆ†æ•°", f"{f1:.4f}")
                
                # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
                cm = confusion_matrix(y_new, y_pred)
                
                fig, ax = plt.subplots(figsize=(8, 6))
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                            xticklabels=['éæˆçŸ¿', 'æˆçŸ¿'],
                            yticklabels=['éæˆçŸ¿', 'æˆçŸ¿'],
                            annot_kws={"size": 14}, ax=ax)
                ax.set_title(f'{model_name} - æ–°æ•°æ®é›†æ··æ·†çŸ©é˜µ', fontsize=14)
                ax.set_ylabel('å®é™…ç±»åˆ«', fontsize=12)
                ax.set_xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
                plt.tight_layout()
                st.pyplot(fig)
                plt.close()
                
                # æ˜¾ç¤ºåˆ†ç±»æŠ¥å‘Š
                with st.expander(f"{model_name} æ–°æ•°æ®é›†åˆ†ç±»æŠ¥å‘Š"):
                    st.text(classification_report(y_new, y_pred, target_names=['éæˆçŸ¿', 'æˆçŸ¿']))
            else:
                # å¯è§†åŒ–é¢„æµ‹ç»“æœåˆ†å¸ƒ
                fig, ax = plt.subplots(figsize=(10, 6))
                sns.countplot(x='Predicted_Label', data=prediction_df, ax=ax)
                ax.set_title(f'{model_name} - é¢„æµ‹ç»“æœåˆ†å¸ƒ', fontsize=16)
                ax.set_xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
                ax.set_ylabel('æ ·æœ¬æ•°é‡', fontsize=12) 
                ax.set_xticklabels(['éæˆçŸ¿', 'æˆçŸ¿'])
                plt.tight_layout()
                st.pyplot(fig)
                plt.close()
                
        except Exception as e:
            st.error(f"å¤„ç† {model_name} æ—¶å‡ºé”™: {str(e)}")

# åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹
def load_saved_models():
    saved_models = {}
    model_files = {
        'XGBoost': 'XGBoost_model.pkl',
        'Random Forest': 'Random_Forest_model.pkl', 
        'SVM': 'SVM_model.pkl',
        'Logistic Regression': 'Logistic_Regression_model.pkl',
        'Neural Network': 'Neural_Network_model.pkl'
    }
    
    for model_name, filename in model_files.items():
        if os.path.exists(filename):
            try:
                model_dict = joblib.load(filename)
                saved_models[model_name] = model_dict
                st.sidebar.success(f"âœ… {model_name} å·²åŠ è½½")
            except Exception as e:
                st.sidebar.warning(f"âš ï¸ {model_name} åŠ è½½å¤±è´¥: {e}")
        else:
            st.sidebar.info(f"ğŸ“ {model_name} æœªè®­ç»ƒ")
    
    return saved_models

# ä¸‹è½½è¯„ä¼°ç»“æœåŠŸèƒ½
def download_evaluation_results(evaluation_results, feature_names):
    if not evaluation_results:
        st.warning("æ²¡æœ‰å¯ä¸‹è½½çš„è¯„ä¼°ç»“æœï¼Œè¯·å…ˆè¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚")
        return
    
    st.subheader("ğŸ“¥ ä¸‹è½½è¯„ä¼°ç»“æœ")
    
    # åˆ›å»ºæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # åˆ›å»ºè¯„ä¼°æŠ¥å‘Š
    report_content, report_timestamp = create_evaluation_report(
        evaluation_results['metrics_df'],
        evaluation_results['rank_df'],
        evaluation_results['models'],
        feature_names,
        evaluation_results['cv_folds']
    )
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # ä¸‹è½½æ€§èƒ½æŒ‡æ ‡è¡¨æ ¼
        st.markdown("### æ€§èƒ½æŒ‡æ ‡è¡¨æ ¼")
        st.markdown(get_table_download_link(
            evaluation_results['metrics_df'], 
            f"model_metrics_{timestamp}.csv", 
            "ğŸ“Š ä¸‹è½½æ€§èƒ½æŒ‡æ ‡è¡¨æ ¼"
        ), unsafe_allow_html=True)
        
        st.markdown(get_table_download_link(
            evaluation_results['rank_df'], 
            f"model_ranks_{timestamp}.csv", 
            "ğŸ† ä¸‹è½½æ¨¡å‹æ’åè¡¨æ ¼"
        ), unsafe_allow_html=True)
    
    with col2:
        # ä¸‹è½½å¯è§†åŒ–å›¾è¡¨
        st.markdown("### å¯è§†åŒ–å›¾è¡¨")
        
        # ROCæ›²çº¿
        if 'roc_fig' in evaluation_results:
            st.markdown(get_image_download_link(
                evaluation_results['roc_fig'],
                f"roc_curves_{timestamp}.png",
                "ğŸ“ˆ ä¸‹è½½ROCæ›²çº¿"
            ), unsafe_allow_html=True)
        
        # PRæ›²çº¿
        if 'pr_fig' in evaluation_results:
            st.markdown(get_image_download_link(
                evaluation_results['pr_fig'],
                f"pr_curves_{timestamp}.png",
                "ğŸ“Š ä¸‹è½½PRæ›²çº¿"
            ), unsafe_allow_html=True)
    
    with col3:
        # ä¸‹è½½æ··æ·†çŸ©é˜µå’Œç‰¹å¾é‡è¦æ€§
        st.markdown("### æ¨¡å‹è¯¦ç»†å›¾è¡¨")
        
        # æ··æ·†çŸ©é˜µ
        if 'confusion_figures' in evaluation_results:
            for model_name, fig in evaluation_results['confusion_figures'].items():
                st.markdown(get_image_download_link(
                    fig,
                    f"confusion_matrix_{model_name}_{timestamp}.png",
                    f"ğŸ¯ ä¸‹è½½{model_name}æ··æ·†çŸ©é˜µ"
                ), unsafe_allow_html=True)
        
        # ç‰¹å¾é‡è¦æ€§
        if 'feature_figures' in evaluation_results:
            for model_name, fig in evaluation_results['feature_figures'].items():
                st.markdown(get_image_download_link(
                    fig,
                    f"feature_importance_{model_name}_{timestamp}.png",
                    f"ğŸ” ä¸‹è½½{model_name}ç‰¹å¾é‡è¦æ€§"
                ), unsafe_allow_html=True)
    
    # ä¸‹è½½å®Œæ•´è¯„ä¼°æŠ¥å‘Š
    st.markdown("---")
    st.markdown("### å®Œæ•´è¯„ä¼°æŠ¥å‘Š")
    st.download_button(
        label="ğŸ“„ ä¸‹è½½å®Œæ•´è¯„ä¼°æŠ¥å‘Š (TXT)",
        data=report_content,
        file_name=f"model_evaluation_report_{timestamp}.txt",
        mime="text/plain"
    )
    
    # æ˜¾ç¤ºæŠ¥å‘Šé¢„è§ˆ
    with st.expander("é¢„è§ˆè¯„ä¼°æŠ¥å‘Š"):
        st.text(report_content)

# ä¸»åº”ç”¨
def main():
    initialize_session_state()
    
    st.title("ğŸ”¬ æœºå™¨å­¦ä¹ åˆ¤åˆ«åˆ†ç±»æ¨¡å‹ç³»ç»Ÿ")
    st.markdown("---")
    
    # ä¾§è¾¹æ å¯¼èˆª - ç›´æ¥æ˜¾ç¤ºäº”ä¸ªåŠŸèƒ½åŒº
    st.sidebar.title("ğŸš€ åŠŸèƒ½åŒºå¯¼èˆª")
    
    # åŠŸèƒ½åŒºæŒ‰é’®
    if st.sidebar.button("ğŸ  é¦–é¡µ", use_container_width=True):
        st.session_state.current_page = "é¦–é¡µ"
    
    if st.sidebar.button("ğŸ“Š æ•°æ®ä¸Šä¼ ", use_container_width=True):
        st.session_state.current_page = "æ•°æ®ä¸Šä¼ "
    
    if st.sidebar.button("ğŸ¤– æ¨¡å‹è®­ç»ƒ", use_container_width=True):
        st.session_state.current_page = "æ¨¡å‹è®­ç»ƒ"
    
    if st.sidebar.button("ğŸ“ˆ æ¨¡å‹è¯„ä¼°", use_container_width=True):
        st.session_state.current_page = "æ¨¡å‹è¯„ä¼°"
    
    if st.sidebar.button("ğŸ”® é¢„æµ‹æ–°æ•°æ®", use_container_width=True):
        st.session_state.current_page = "é¢„æµ‹æ–°æ•°æ®"
    
    if st.sidebar.button("âš™ï¸ å‚æ•°è®¾ç½®", use_container_width=True):
        st.session_state.current_page = "å‚æ•°è®¾ç½®"
    
    st.sidebar.markdown("---")
    
    # äº¤å‰éªŒè¯è®¾ç½®
    st.sidebar.subheader("ğŸ”§ äº¤å‰éªŒè¯è®¾ç½®")
    cv_folds = st.sidebar.radio(
        "é€‰æ‹©äº¤å‰éªŒè¯æŠ˜æ•°",
        [5, 10],
        index=0 if st.session_state.cv_folds == 5 else 1,
        key="cv_folds_sidebar"
    )
    st.session_state.cv_folds = cv_folds
    st.sidebar.info(f"å½“å‰ä½¿ç”¨: {cv_folds}æŠ˜äº¤å‰éªŒè¯")
    
    st.sidebar.markdown("---")
    
    # åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹
    st.sidebar.subheader("ğŸ“ å·²ä¿å­˜æ¨¡å‹")
    saved_models = load_saved_models()
    if saved_models:
        st.session_state.trained_models.update(saved_models)
        st.sidebar.success(f"å·²åŠ è½½ {len(saved_models)} ä¸ªæ¨¡å‹")
    
    # æ¸…ç©ºæ•°æ®æŒ‰é’®
    st.sidebar.markdown("---")
    if st.sidebar.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ®", type="secondary"):
        # é‡ç½®æ‰€æœ‰çŠ¶æ€
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        # é‡æ–°åˆå§‹åŒ–
        initialize_session_state()
        st.rerun()
    
    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ“Š å½“å‰çŠ¶æ€")
    
    status_col1, status_col2 = st.sidebar.columns(2)
    with status_col1:
        st.metric("æ•°æ®çŠ¶æ€", "âœ…" if st.session_state.data_loaded else "âŒ")
    with status_col2:
        trained_count = len(st.session_state.trained_models)
        st.metric("æ¨¡å‹æ•°é‡", f"{trained_count}/5")
    
    # æ ¹æ®å½“å‰é¡µé¢æ˜¾ç¤ºå†…å®¹
    current_page = st.session_state.current_page
    
    # é¦–é¡µ
    if current_page == "é¦–é¡µ":
        st.header("æ¬¢è¿ä½¿ç”¨æœºå™¨å­¦ä¹ åˆ¤åˆ«åˆ†ç±»æ¨¡å‹ç³»ç»Ÿ")
        st.markdown("""
        ### ğŸ¯ ç³»ç»ŸåŠŸèƒ½
        
        **ğŸ“Š æ•°æ®ä¸Šä¼ ** - ä¸Šä¼ é”†çŸ³æ•°æ®æ–‡ä»¶å¹¶è¿›è¡Œé¢„å¤„ç†
        
        **ğŸ¤– æ¨¡å‹è®­ç»ƒ** - é€‰æ‹©å¹¶è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹
        - XGBoost
        - Random Forest  
        - SVM
        - Logistic Regression
        - Neural Network
        
        **ğŸ“ˆ æ¨¡å‹è¯„ä¼°** - è¯„ä¼°æ¨¡å‹æ€§èƒ½å¹¶å¯è§†åŒ–ç»“æœ
        
        **ğŸ”® é¢„æµ‹æ–°æ•°æ®** - ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹
        
        **âš™ï¸ å‚æ•°è®¾ç½®** - è‡ªå®šä¹‰æ¨¡å‹å‚æ•°
        
        ### ğŸš€ ä½¿ç”¨æµç¨‹
        1. åœ¨"æ•°æ®ä¸Šä¼ "é¡µé¢ä¸Šä¼ æ‚¨çš„æ•°æ®
        2. åœ¨"æ¨¡å‹è®­ç»ƒ"é¡µé¢é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹
        3. åœ¨"æ¨¡å‹è¯„ä¼°"é¡µé¢æŸ¥çœ‹æ¨¡å‹æ€§èƒ½
        4. åœ¨"é¢„æµ‹æ–°æ•°æ®"é¡µé¢ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
        
        ### ğŸ’¡ æ¸©é¦¨æç¤º
        - æ¯ä¸ªåŠŸèƒ½åŒºçš„æ“ä½œç»“æœéƒ½ä¼šä¿ç•™ï¼Œæ–¹ä¾¿æ‚¨å¯¹æ¯”æŸ¥çœ‹
        - å¯ä»¥éšæ—¶ä½¿ç”¨ä¾§è¾¹æ çš„"æ¸…ç©ºæ‰€æœ‰æ•°æ®"é‡æ–°å¼€å§‹
        - è®­ç»ƒå¥½çš„æ¨¡å‹ä¼šè‡ªåŠ¨ä¿å­˜ï¼Œä¸‹æ¬¡å¯ç›´æ¥ä½¿ç”¨
        - æ¨¡å‹è¯„ä¼°ç»“æœå¯ä»¥ä¸‹è½½ä¿å­˜
        - å¯åœ¨ä¾§è¾¹æ é€‰æ‹©5æŠ˜æˆ–10æŠ˜äº¤å‰éªŒè¯
        """)
        
        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
        col1, col2, col3 = st.columns(3)
        with col1:
            status = "âœ… å·²åŠ è½½" if st.session_state.data_loaded else "âŒ æœªåŠ è½½"
            st.metric("æ•°æ®çŠ¶æ€", status)
        with col2:
            trained_count = len(st.session_state.trained_models)
            st.metric("å·²è®­ç»ƒæ¨¡å‹", f"{trained_count}/5")
        with col3:
            status = "âœ… å®Œæˆ" if st.session_state.training_complete else "â³ å¾…è®­ç»ƒ"
            st.metric("è®­ç»ƒçŠ¶æ€", status)
        
        # æ˜¾ç¤ºäº¤å‰éªŒè¯è®¾ç½®
        st.info(f"å½“å‰äº¤å‰éªŒè¯è®¾ç½®: **{st.session_state.cv_folds}æŠ˜äº¤å‰éªŒè¯**")
    
    # æ•°æ®ä¸Šä¼ é¡µé¢
    elif current_page == "æ•°æ®ä¸Šä¼ ":
        st.header("ğŸ“Š æ•°æ®ä¸Šä¼ ")
        st.markdown("ä¸Šä¼ æ‚¨çš„é”†çŸ³æ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒExcelå’ŒCSVæ ¼å¼ï¼‰")
        
        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
        if st.session_state.data_loaded:
            st.success("âœ… æ•°æ®å·²åŠ è½½å¹¶é¢„å¤„ç†å®Œæˆ")
            st.write(f"- è®­ç»ƒé›†å¤§å°: {st.session_state.X_train.shape[0]}")
            st.write(f"- æµ‹è¯•é›†å¤§å°: {st.session_state.X_test.shape[0]}")
            st.write(f"- ç‰¹å¾æ•°é‡: {len(st.session_state.feature_names)}")
        
        uploaded_file = st.file_uploader("é€‰æ‹©æ–‡ä»¶", type=['xlsx', 'csv'])
        
        if uploaded_file is not None:
            st.session_state.uploaded_file = uploaded_file
            
            # æ˜¾ç¤ºæ•°æ®ä¿¡æ¯
            try:
                if uploaded_file.name.endswith('.xlsx'):
                    data = pd.read_excel(uploaded_file)
                else:
                    data = pd.read_csv(uploaded_file)
                
                st.write("**æ•°æ®é¢„è§ˆ:**")
                st.dataframe(data.head())
                
                col1, col2 = st.columns(2)
                with col1:
                    st.write("**æ•°æ®ä¿¡æ¯:**")
                    st.write(f"- æ•°æ®å½¢çŠ¶: {data.shape}")
                    st.write(f"- ç‰¹å¾æ•°é‡: {data.shape[1]-1}")
                    st.write(f"- æ ·æœ¬æ•°é‡: {data.shape[0]}")
                
                with col2:
                    if 'Label' in data.columns:
                        st.write("**æ ‡ç­¾åˆ†å¸ƒ:**")
                        label_counts = data['Label'].value_counts()
                        st.write(label_counts)
                
                if 'Label' in data.columns:
                    # å¯è§†åŒ–æ ‡ç­¾åˆ†å¸ƒ
                    fig, ax = plt.subplots(figsize=(8, 6))
                    label_counts = data['Label'].value_counts()
                    label_counts.plot(kind='bar', ax=ax)
                    ax.set_title('æ ‡ç­¾åˆ†å¸ƒ', fontsize=16)
                    ax.set_xlabel('æ ‡ç­¾', fontsize=12)
                    ax.set_ylabel('æ•°é‡', fontsize=12)
                    ax.set_xticklabels(['éæˆçŸ¿', 'æˆçŸ¿'], rotation=0)
                    plt.tight_layout()
                    st.pyplot(fig)
                    plt.close()
                
                # é¢„å¤„ç†æ•°æ®
                if st.button("å¼€å§‹é¢„å¤„ç†æ•°æ®", type="primary"):
                    with st.spinner("æ­£åœ¨é¢„å¤„ç†æ•°æ®..."):
                        X_train, X_test, y_train, y_test, feature_names, data_shape = load_and_preprocess_data(uploaded_file)
                        
                        if X_train is not None:
                            st.session_state.X_train = X_train
                            st.session_state.X_test = X_test
                            st.session_state.y_train = y_train
                            st.session_state.y_test = y_test
                            st.session_state.feature_names = feature_names
                            st.session_state.data_loaded = True
                            st.session_state.data_preprocessed = True
                            
                            st.success("æ•°æ®é¢„å¤„ç†å®Œæˆ!")
                            st.write(f"- è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
                            st.write(f"- æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
                            st.write(f"- ç‰¹å¾æ•°é‡: {len(feature_names)}")
                            st.write(f"- ç‰¹å¾åˆ—è¡¨: {', '.join(feature_names)}")
            except Exception as e:
                st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    # æ¨¡å‹è®­ç»ƒé¡µé¢
    elif current_page == "æ¨¡å‹è®­ç»ƒ":
        st.header("ğŸ¤– æ¨¡å‹è®­ç»ƒ")
        
        # æ˜¾ç¤ºäº¤å‰éªŒè¯è®¾ç½®
        st.info(f"å½“å‰äº¤å‰éªŒè¯è®¾ç½®: **{st.session_state.cv_folds}æŠ˜äº¤å‰éªŒè¯**")
        
        if not st.session_state.data_loaded:
            st.warning("è¯·å…ˆä¸Šä¼ å¹¶é¢„å¤„ç†æ•°æ®!")
            return
        
        st.write("**é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹:**")
        
        # æ¨¡å‹é€‰æ‹©
        model_options = ['XGBoost', 'Random Forest', 'SVM', 'Logistic Regression', 'Neural Network']
        selected_models = st.multiselect(
            "é€‰æ‹©æ¨¡å‹",
            model_options,
            default=model_options
        )
        
        # æœç´¢æ–¹æ³•é€‰æ‹©
        search_method = st.radio(
            "é€‰æ‹©å‚æ•°æœç´¢æ–¹æ³•",
            ["éšæœºæœç´¢", "ç½‘æ ¼æœç´¢", "è‡ªå®šä¹‰å‚æ•°"]
        )
        
        # æ˜¾ç¤ºå·²ä¿å­˜çš„æ¨¡å‹
        if st.session_state.trained_models:
            st.info(f"ğŸ“ å·²åŠ è½½ {len(st.session_state.trained_models)} ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹")
            trained_list = list(st.session_state.trained_models.keys())
            st.write(f"å·²è®­ç»ƒæ¨¡å‹: {', '.join(trained_list)}")
        
        # è®­ç»ƒé€‰é¡¹
        col1, col2 = st.columns(2)
        with col1:
            use_existing = st.checkbox("ä½¿ç”¨å·²ä¿å­˜çš„æ¨¡å‹ï¼ˆå¦‚å­˜åœ¨ï¼‰", value=True)
        with col2:
            retrain = st.checkbox("é‡æ–°è®­ç»ƒé€‰ä¸­çš„æ¨¡å‹", value=False)
        
        # è®­ç»ƒæŒ‰é’®
        if st.button("å¼€å§‹è®­ç»ƒæ¨¡å‹", type="primary"):
            if not selected_models:
                st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹!")
                return
            
            with st.spinner("æ­£åœ¨è®­ç»ƒæ¨¡å‹..."):
                # ç¡®å®šè¦è®­ç»ƒçš„æ¨¡å‹
                models_to_train = selected_models
                if use_existing and not retrain:
                    # æ’é™¤å·²å­˜åœ¨çš„æ¨¡å‹
                    existing_models = list(st.session_state.trained_models.keys())
                    models_to_train = [model for model in selected_models if model not in existing_models]
                    
                    if not models_to_train:
                        st.info("æ‰€æœ‰é€‰ä¸­çš„æ¨¡å‹éƒ½å·²è®­ç»ƒå®Œæˆï¼Œä½¿ç”¨ç°æœ‰æ¨¡å‹ã€‚")
                    else:
                        st.info(f"å°†è®­ç»ƒæ–°æ¨¡å‹: {', '.join(models_to_train)}")
                
                if models_to_train or retrain:
                    # è·å–è‡ªå®šä¹‰å‚æ•°
                    custom_params = {}
                    if search_method == "è‡ªå®šä¹‰å‚æ•°":
                        custom_params = st.session_state.get('custom_params', {})
                    
                    trained_models = train_models(
                        st.session_state.X_train, 
                        st.session_state.y_train, 
                        models_to_train if not retrain else selected_models, 
                        search_method,
                        st.session_state.cv_folds,
                        custom_params
                    )
                    
                    # æ›´æ–°session state
                    st.session_state.trained_models.update(trained_models)
                    st.success(f"æ¨¡å‹è®­ç»ƒå®Œæˆ! å…±è®­ç»ƒ {len(trained_models)} ä¸ªæ¨¡å‹")
                else:
                    st.success("ä½¿ç”¨ç°æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹")
    
    # æ¨¡å‹è¯„ä¼°é¡µé¢
    elif current_page == "æ¨¡å‹è¯„ä¼°":
        st.header("ğŸ“ˆ æ¨¡å‹è¯„ä¼°")
        
        # æ˜¾ç¤ºäº¤å‰éªŒè¯è®¾ç½®
        st.info(f"å½“å‰äº¤å‰éªŒè¯è®¾ç½®: **{st.session_state.cv_folds}æŠ˜äº¤å‰éªŒè¯**")
        
        if not st.session_state.trained_models:
            st.warning("è¯·å…ˆè®­ç»ƒæ¨¡å‹!")
            return
        
        # æ˜¾ç¤ºå·²å®Œæˆçš„è¯„ä¼°
        if st.session_state.evaluation_done:
            st.success("âœ… æ¨¡å‹è¯„ä¼°å·²å®Œæˆ")
            st.write("ä»¥ä¸‹æ˜¯ä¹‹å‰çš„è¯„ä¼°ç»“æœ:")
            
            # ç›´æ¥æ˜¾ç¤ºä¹‹å‰çš„è¯„ä¼°ç»“æœ
            evaluate_and_visualize(
                st.session_state.trained_models,
                st.session_state.X_train,
                st.session_state.X_test,
                st.session_state.y_train,
                st.session_state.y_test,
                st.session_state.feature_names,
                st.session_state.cv_folds
            )
            
            # ä¸‹è½½è¯„ä¼°ç»“æœ
            if st.session_state.evaluation_results:
                download_evaluation_results(
                    st.session_state.evaluation_results,
                    st.session_state.feature_names
                )
        else:
            # è¯„ä¼°æŒ‰é’®
            if st.button("å¼€å§‹è¯„ä¼°", type="primary"):
                with st.spinner("æ­£åœ¨è¯„ä¼°æ¨¡å‹..."):
                    metrics, rank_df, evaluation_results = evaluate_and_visualize(
                        st.session_state.trained_models,
                        st.session_state.X_train,
                        st.session_state.X_test,
                        st.session_state.y_train,
                        st.session_state.y_test,
                        st.session_state.feature_names,
                        st.session_state.cv_folds
                    )
                    
                    st.session_state.evaluation_results = evaluation_results
                    st.success("æ¨¡å‹è¯„ä¼°å®Œæˆ!")
            
            # ä¸‹è½½è¯„ä¼°ç»“æœ
            if st.session_state.evaluation_results:
                download_evaluation_results(
                    st.session_state.evaluation_results,
                    st.session_state.feature_names
                )
    
    # é¢„æµ‹æ–°æ•°æ®é¡µé¢
    elif current_page == "é¢„æµ‹æ–°æ•°æ®":
        st.header("ğŸ”® é¢„æµ‹æ–°æ•°æ®")
        
        if not st.session_state.trained_models:
            st.warning("è¯·å…ˆè®­ç»ƒæ¨¡å‹!")
            return
        
        st.write("ä¸Šä¼ æ–°æ•°æ®è¿›è¡Œé¢„æµ‹")
        new_data_file = st.file_uploader("é€‰æ‹©æ–°æ•°æ®æ–‡ä»¶", type=['xlsx', 'csv'], key="new_data")
        
        if new_data_file is not None:
            # é€‰æ‹©ç”¨äºé¢„æµ‹çš„æ¨¡å‹
            trained_model_names = list(st.session_state.trained_models.keys())
            selected_models_for_prediction = st.multiselect(
                "é€‰æ‹©ç”¨äºé¢„æµ‹çš„æ¨¡å‹",
                trained_model_names,
                default=trained_model_names
            )
            
            if st.button("å¼€å§‹é¢„æµ‹", type="primary"):
                if not selected_models_for_prediction:
                    st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹!")
                    return
                
                with st.spinner("æ­£åœ¨è¿›è¡Œé¢„æµ‹..."):
                    predict_new_dataset(
                        st.session_state.trained_models,
                        new_data_file,
                        selected_models_for_prediction
                    )
    
    # å‚æ•°è®¾ç½®é¡µé¢
    elif current_page == "å‚æ•°è®¾ç½®":
        st.header("âš™ï¸ å‚æ•°è®¾ç½®")
        
        st.info("åœ¨è¿™é‡Œè®¾ç½®è‡ªå®šä¹‰æ¨¡å‹å‚æ•°")
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹è®¾ç½®å‚æ•°
        model_options = ['XGBoost', 'Random Forest', 'SVM', 'Logistic Regression', 'Neural Network']
        
        custom_params = st.session_state.get('custom_params', {})
        
        for model in model_options:
            with st.expander(f"{model} å‚æ•°"):
                if model == 'XGBoost':
                    n_estimators = st.slider("n_estimators", 50, 300, 100, key=f"xgb_n_est")
                    max_depth = st.slider("max_depth", 3, 10, 6, key=f"xgb_depth")
                    learning_rate = st.slider("learning_rate", 0.01, 0.3, 0.1, key=f"xgb_lr")
                    subsample = st.slider("subsample", 0.6, 1.0, 0.8, key=f"xgb_sub")
                    colsample_bytree = st.slider("colsample_bytree", 0.6, 1.0, 0.8, key=f"xgb_col")
                    
                    custom_params[model] = {
                        'n_estimators': n_estimators,
                        'max_depth': max_depth,
                        'learning_rate': learning_rate,
                        'subsample': subsample,
                        'colsample_bytree': colsample_bytree
                    }
                
                elif model == 'Random Forest':
                    n_estimators = st.slider("n_estimators", 50, 300, 100, key=f"rf_n_est")
                    max_depth = st.slider("max_depth", 3, 20, 10, key=f"rf_depth")
                    min_samples_split = st.slider("min_samples_split", 2, 10, 2, key=f"rf_split")
                    min_samples_leaf = st.slider("min_samples_leaf", 1, 5, 1, key=f"rf_leaf")
                    
                    custom_params[model] = {
                        'n_estimators': n_estimators,
                        'max_depth': max_depth,
                        'min_samples_split': min_samples_split,
                        'min_samples_leaf': min_samples_leaf
                    }
                
                elif model == 'SVM':
                    C = st.slider("C", 0.1, 10.0, 1.0, key=f"svm_c")
                    gamma = st.slider("gamma", 0.01, 1.0, 0.1, key=f"svm_gamma")
                    kernel = st.selectbox("kernel", ['linear', 'rbf', 'poly'], key=f"svm_kernel")
                    
                    custom_params[model] = {
                        'C': C,
                        'gamma': gamma,
                        'kernel': kernel
                    }
                
                elif model == 'Logistic Regression':
                    C = st.slider("C", 0.1, 10.0, 1.0, key=f"lr_c")
                    penalty = st.selectbox("penalty", ['l1', 'l2'], key=f"lr_penalty")
                    
                    custom_params[model] = {
                        'C': C,
                        'penalty': penalty,
                        'solver': 'liblinear'
                    }
                
                elif model == 'Neural Network':
                    hidden_layer_sizes = st.selectbox(
                        "hidden_layer_sizes", 
                        [(50,), (100,), (50, 30), (100, 50)],
                        format_func=lambda x: f"{x}",
                        key=f"nn_layers"
                    )
                    alpha = st.slider("alpha", 0.0001, 0.1, 0.001, key=f"nn_alpha")
                    activation = st.selectbox("activation", ['relu', 'tanh'], key=f"nn_act")
                    learning_rate_init = st.slider("learning_rate_init", 0.001, 0.01, 0.001, key=f"nn_lr")
                    
                    custom_params[model] = {
                        'hidden_layer_sizes': hidden_layer_sizes,
                        'alpha': alpha,
                        'activation': activation,
                        'learning_rate_init': learning_rate_init
                    }
        
        # ä¿å­˜è‡ªå®šä¹‰å‚æ•°åˆ°session state
        st.session_state.custom_params = custom_params
        
        if st.button("ä¿å­˜å‚æ•°", type="primary"):
            st.success("å‚æ•°å·²ä¿å­˜! ç°åœ¨å¯ä»¥åœ¨æ¨¡å‹è®­ç»ƒé¡µé¢ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°äº†ã€‚")

if __name__ == "__main__":
    main()